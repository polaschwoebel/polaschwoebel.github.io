<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-10-20T09:58:09+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Pola Schwöbel</title><subtitle>A simple, whitespace, helvetica based portfolio theme.
</subtitle><entry><title type="html">thoughts on algorithmic fairness</title><link href="http://localhost:4000/2020/10/18/title.html" rel="alternate" type="text/html" title="thoughts on algorithmic fairness" /><published>2020-10-18T18:44:00+02:00</published><updated>2020-10-18T18:44:00+02:00</updated><id>http://localhost:4000/2020/10/18/title</id><content type="html" xml:base="http://localhost:4000/2020/10/18/title.html">&lt;p&gt;&lt;strong&gt;Introduction and Overview&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;As research in machine learning is rapidly progressing, more and more decisions that affect our lives are being automated. These can be rather trivial decisions (e.g. which coffee brand is advertised to me online), but increasingly also more essential judgements are being handed over to algorithms, for example whether we are eligible for a bank loan in order to buy an apartment. If algorithms fail in such a scenario, the consequences for the individual are dire.
 The COMPAS case is arguably the most prominent example of such algorithmic decision-making failing in systematic ways - in an extremely high stakes scenario.  The COMPAS (short for Correctional Offender Management Profiling for Alternative Sanctions) algorithm is a predictive justice tool by for-profit company Northpointe used in the US to evaluate risk of recidivism, i.e. how likely a criminal is to re-offend. The tool is used by judges in their decision making on parole, sentence length etc.  Investigative journalists at ProPublica discovered in their famous 2016 study [1] how Northpointe’s algorithm is racially biased: it more often underestimates risk of re-offending for white people and overestimates it for African Americans.&lt;/p&gt;

&lt;div class=&quot;img&quot;&gt;
	&lt;img class=&quot;col three&quot; src=&quot;/img/compas_scores.png&quot; /&gt;
&lt;/div&gt;
&lt;div class=&quot;col three caption&quot;&gt;
	Error rates for the COMPAS algorithm.
&lt;/div&gt;

&lt;p&gt;Pro-publica’s work brought the problem of Algorithmic Fairness and Bias into the spotlight and started a heated debate within the general public as well as the scientific community. 
Computer scientists (along with statisticians, law scholars and political and social scientists) responded quickly with attempts to fix those biases. A community on Algorithmic Fairness formed and is steadily growing, publishing a large body of research for example through ACM’s conference on Fairness, Accountability and Transparency being held yearly since 2018 [2].&lt;/p&gt;

&lt;p&gt;As scientists, much of their early efforts went into defining what is actually means for an algorithm to be “fair”. Such fairness metrics can be roughly divided into three categories: criteria based on group fairness, individual fairness and causal criteria (see info box).&lt;/p&gt;

&lt;div style=&quot;background:#efefef; padding:2em; margin: 2em 2em 2em;&quot;&gt;
&lt;b&gt; Example of Fairness metrics for the example of university admission &lt;/b&gt; &lt;br /&gt;


For illustration, we use gender as the “protected attribute” with respect to which fairness is to be analysed.  &lt;br /&gt;&lt;br /&gt;


&lt;b&gt; Individual Fairness: &lt;/b&gt; “Similar individuals should have similar outcomes”. If student A and B have the same GPA, they should either both be admitted or both not be admitted, no matter their gender. &lt;br /&gt;
&lt;b&gt; Group Fairness: &lt;/b&gt; “ Demographic groups should have similar outcomes”. An equal amount of men and women should be admitted, even if this means different GPA thresholds for male and female students. &lt;br /&gt;
&lt;b&gt; Causal Fairness criteria: &lt;/b&gt; “The protected attribute should not influence the decision”. The outcome doesn’t matter as long as the gender has not explicitly been used to arrive at the decision.
&lt;/div&gt;

&lt;p&gt;Researchers then go on to think about ways to achieve such fairness criteria. One approach is to manipulate an algorithm’s training data by upsampling any underrepresented groups in the dataset. Alternatively, one can modify the algorithm itself. For the above example of college admission, this could mean decreasing the required GPA for a certain subpopulation in order to obtain a diverse student body.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Criticism of these approaches&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Lately, this type of work has faced a fair amount of criticism. Two arguments come up repeatedly: 
Firstly, there is what I call practical criticism: It’s impossible to quantify a complex concept like fairness, and interestingly, even if we seem to succeed, we often produce inconsistencies between the different fairness criteria discussed above. This can be formally proven in incompatibility theorems like the ones presented in [3]. Here’s an intuitive argument for why individual and group fairness can contradict each other: Imagine a university is admitting students solely based on their final high school GPA. Women had slightly higher grades on average. If we try to respect &lt;em&gt;individual fairness&lt;/em&gt; our admission algorithm could simply be a threshold, grade &amp;gt; t, applied to each student in the exact same way. Because of the difference in grade distribution, this would mean more women than men admitted - a clash with &lt;em&gt;group fairness&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Computational, quantitative methods aren’t sufficient to solve deep ethical problems. We have tried since at least the 17th century, when Leibniz attempted to formalize a calculus of reason that would allow us to solve any argument, especially a moral one, by pure logic deduction:   &lt;/p&gt;
&lt;blockquote&gt;
	“The only way to rectify our reasonings is to make them as tangible as those of the 
  Mathematicians, so that we can find our error at a glance, and when there are
 	disputes  among persons, we can simply say: 
 Let us calculate, without further ado, to see who is right.” [4]
&lt;/blockquote&gt;
&lt;p&gt;Yet we disagree about the most basic questions - a richer language than what the formalisms of mathematics provide seems to be necessary to encapsulate the ambiguities and, sometimes, contradictions inherent to moral discourse.&lt;/p&gt;

&lt;p&gt;Secondly, the algorithmic fairness community is accused of lobbying for big tech. Large corporations are very interested in the topic: Famously, Google founded (and immediately shut down) an ethics board in 2019 [5] but continues to work on the topic [6]. Other examples are their AI daughter company DeepMind with its Ethics &amp;amp; Society group [7] as well as Microsoft’s efforts [8]. All of these companies sponsored the above-mentioned ACM conference on Fairness, Accountability and Transparency in 2020 [9], and it’s worth taking a closer look at their motives.
Certainly, many of the individuals working on such questions in a corporate setting do so, because they acknowledge the huge responsibility and importance of their work and are committed to working on safe and ethical technologies. As profit-seeking companies, however, they certainly have other driving forces as well, an important one being their reputation. “White-washing” their algorithms and attempting to turn them into ethical technologies, however, has another major advantage for big tech companies: By performing small technical adjustments to their products, they can hope to circumvent regulation through laws. As Rodrigo Ochigame phrases it: “The majority of well-funded work on “ethical AI” is aligned with the tech lobby’s agenda: to voluntarily or moderately adjust, rather than legally restrict, the deployment of controversial technologies” [10]. Academia is at danger of becoming complicit in this: “Silicon Valley’s vigorous promotion of “ethical AI” has constituted a strategic lobbying effort, one that has enrolled academia to legitimize itself.” [10]&lt;/p&gt;

&lt;p&gt;He argues that this is not a new phenomenon. Historically, socially unjust decision procedures have often been justified with statistical objectivity. [11] As an example, credit scoring (i.e. banks evaluating whether or not an individual is eligible for a bank loan) used to be based on personal interviews which were supposed to evaluate an individual’s ‘character’. An outcome of such an interview is of course influenced by the interviewer’s individual opinions and thus very vulnerable to possible biases they hold w.r.t. race or class. In the 1960s, such interviews started to be replaced with statistical calculations of credit scores, which inherited their human designer’s biases. When a ban of credit “scorecards” was proposed in the 1970s within the framework of anti-discrimination legislation, their producer Fair, Isaac &amp;amp; Company argued the statistical objectivity of such methods in order to demonstrate compliance with the definition of fairness in the law [12]. Like today, by reducing issues of discrimination and injustice to statistical calculations, we are at danger of cutting out important aspects of such complex moral and political questions, and are possibly standing in the way of urgently necessary regulation of automated decision making technology.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A defense of our attempts&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Despite this valid criticism, I find technical work within algorithmic fairness valuable. It is useful for researchers to try and morally evaluate their work in order to make sure it aligns with their values. Of course, as computer scientists, quantitative methods are the tools most accessible to us, and I do think there is great value in phrasing ethical questions in the language of our community. This should be done avoiding the reductionist trap - always having in mind that ours is just one way to look at the problem. 
Fair ML research should not be carried out as an alternative to solid law-making efforts. Rather, as computer scientists, logicians, statisticians and alike, we should aim to support such efforts, and to translate between social science, law, philosophy and the computational sciences. An example of this “translation” approach is Martin Mose Bentzen’s work on formalizing classic philosophical concepts for use in robots, e.g. in his paper “A Formalization of Kant’s Second Formulation of the Categorical Imperative” [12].&lt;/p&gt;

&lt;p&gt;The discussion of whether or not research on ethical AI and fair machine learning has its place can be embedded in the much larger discussion about the moral responsibility of science. Are science and technology neutral and their moral value purely determined by some application or user, or do we as scientists carry a responsibility? 
&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h4 id=&quot;references&quot;&gt;References&lt;/h4&gt;
&lt;p&gt;&lt;small&gt;
[1] &lt;a href=&quot;https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing&quot;&gt;https://www.propublica.org/article/ machine-bias-risk-assessments-in-criminal-sentencing&lt;/a&gt;  &lt;br /&gt;
[2] &lt;a href=&quot;https://facctconference.org&quot;&gt;https://facctconference.org&lt;/a&gt;  &lt;br /&gt;
[3] Dwork, Cynthia, et al. “Fairness through awareness.” Proceedings of the 3rd innovations in theoretical computer science conference. 2012.  &lt;br /&gt;
[4] Leibniz, G. W. (1951), Leibniz: Selections, P. P. Wiener (Ed. Trans.), New York: Scribner, p. 51. – cited from &lt;a href=&quot;https://publicdomainreview.org/essay/let-us-calculate-leibniz-llull-and-the-computational-imagination#fn13&quot;&gt; https://publicdomainreview.org/essay/let-us-calculate-leibniz-llull-and-the-computational-imagination#fn13&lt;/a&gt;  &lt;br /&gt;
[5] &lt;a href=&quot;https://www.bbc.com/news/technology-47825833&quot;&gt;https://www.bbc.com/news/technology-47825833&lt;/a&gt;  &lt;br /&gt;
[6] &lt;a href=&quot;https://developers.google.com/machine-learning/fairness-overview&quot;&gt;https://developers.google.com/machine-learning/fairness-overview&lt;/a&gt;  &lt;br /&gt;
[7] &lt;a href=&quot;https://deepmind.com/about/ethics-and-society&quot;&gt;https://deepmind.com/about/ethics-and-society&lt;/a&gt;  &lt;br /&gt;
[8] &lt;a href=&quot;https://www.microsoft.com/en-us/ai/responsible-ai&quot;&gt;https://www.microsoft.com/en-us/ai/responsible-ai&lt;/a&gt;  &lt;br /&gt;
[9] &lt;a href=&quot;https://facctconference.org/2020/sponsorship.html&quot;&gt;https://facctconference.org/2020/sponsorship.html&lt;/a&gt;  &lt;br /&gt;
[10] &lt;a href=&quot;https://theintercept.com/2019/12/20/mit-ethical-ai-artificial-intelligence/&quot;&gt;https://theintercept.com/2019/12/20/mit-ethical-ai-artificial-intelligence/&lt;/a&gt;  &lt;br /&gt;
[11] Martha A. Poon, “What Lenders See: A History of the Fair Isaac Scorecard” (PhD diss., UC San Diego, 2012), 167–214.  &lt;br /&gt;
[12] Lindner, Felix, and Martin Mose Bentzen. “A Formalization of Kant’s Second Formulation of the Categorical Imperative.” arXiv preprint arXiv:1801.03160 (2018).  &lt;br /&gt;
&lt;/small&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">Introduction and Overview</summary></entry><entry><title type="html">new paper</title><link href="http://localhost:4000/2020/07/10/title.html" rel="alternate" type="text/html" title="new paper" /><published>2020-07-10T11:26:00+02:00</published><updated>2020-07-10T11:26:00+02:00</updated><id>http://localhost:4000/2020/07/10/title</id><content type="html" xml:base="http://localhost:4000/2020/07/10/title.html">&lt;p&gt;You can find “Probabilistic Spatial Transformers for Bayesian Data Augmentation” &lt;a href=&quot;https://arxiv.org/pdf/2004.03637.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">You can find “Probabilistic Spatial Transformers for Bayesian Data Augmentation” here.</summary></entry><entry><title type="html">talk at AI expo</title><link href="http://localhost:4000/2019/10/20/title.html" rel="alternate" type="text/html" title="talk at AI expo" /><published>2019-10-20T14:11:00+02:00</published><updated>2019-10-20T14:11:00+02:00</updated><id>http://localhost:4000/2019/10/20/title</id><content type="html" xml:base="http://localhost:4000/2019/10/20/title.html">&lt;p&gt;Last month I was invited to give a talk at the AI Expo at &lt;a href=&quot;hhttps://www.skylab.dtu.dk&quot;&gt;Skylab&lt;/a&gt;, DTU’s lovely space for student innovation and entrepreneurship. I gave a quick overview over the field of Algorithmic Fairness and spoke a bit about my own research. The slides can be found &lt;a href=&quot;/img/Bias_in_bias_out.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;img&quot;&gt;
	&lt;img class=&quot;col three&quot; src=&quot;/img/skylab.jpg&quot; /&gt;
&lt;/div&gt;
&lt;div class=&quot;col three caption&quot;&gt;
	200+ people showed up and asked a lot of smart questions after my talk. Thanks!
&lt;/div&gt;</content><author><name></name></author><summary type="html">Last month I was invited to give a talk at the AI Expo at Skylab, DTU’s lovely space for student innovation and entrepreneurship. I gave a quick overview over the field of Algorithmic Fairness and spoke a bit about my own research. The slides can be found here.</summary></entry><entry><title type="html">summer school organization</title><link href="http://localhost:4000/2019/09/06/title.html" rel="alternate" type="text/html" title="summer school organization" /><published>2019-09-06T18:11:00+02:00</published><updated>2019-09-06T18:11:00+02:00</updated><id>http://localhost:4000/2019/09/06/title</id><content type="html" xml:base="http://localhost:4000/2019/09/06/title.html">&lt;p&gt;My section holds a summer school on Advanced Topics in Machine Learning every year. For the first time it was organized by a PhD student - me! I chose the topic “Fairness and Machine Learning” and hosted the &lt;a href=&quot;http://www2.compute.dtu.dk/courses/02901/&quot;&gt; event&lt;/a&gt; at DTU from August 26th - 30th.&lt;/p&gt;

&lt;p&gt;Algorithmic fairness is something I am interested in personally, and many more people are. A quite vibrant research field within computer science and ML in particular has emerged during the last 8 years or so. People are interested in measuring how “fair” algorithms are, for example by evaluating whether their decisions are equally advantageous for men and women, or whether the same prediction accuracy is achieved across different demographic groups. Once algorithmic bias is detected, researchers try to develop techniques to overcome it.&lt;/p&gt;

&lt;p&gt;Measuring fairness in such a quantitative way is of course a hard thing to do. Fairness can mean different things depending on context, and some of the definitions contradict each other. While we as computer scientists cannot solve those societal questions, I find it very useful to try and operationalize some of our ideas of ethics and translate them into formulas and code, thereby making them accessible for the technical community to work with. Putting something into maths is also a really good way to force yourself to be precise about what exactly you mean when you say an algorithm is (un-)fair.&lt;/p&gt;

&lt;p&gt;Our great speakers gave lectures and tutorials during the five course days. In the evening there were social events, e.g. a fantastic meetup where start up people talked about the implementation of the theoretical concepts we had been discussing. Super interesting week packed with smart people discussing difficult problems (and a lot of hard work pulling all the strings). Looking forward to next year!&lt;/p&gt;

&lt;div class=&quot;img&quot;&gt;
	&lt;img class=&quot;col three&quot; src=&quot;/img/silvia_chiappa.jpg&quot; /&gt;
&lt;/div&gt;
&lt;div class=&quot;col three caption&quot;&gt;
	A bad picture of a great talk: Silvia Chiappa from Deepmind talking about Algorithmic Fairness and Causal Inference.
&lt;/div&gt;</content><author><name></name></author><summary type="html">My section holds a summer school on Advanced Topics in Machine Learning every year. For the first time it was organized by a PhD student - me! I chose the topic “Fairness and Machine Learning” and hosted the event at DTU from August 26th - 30th.</summary></entry><entry><title type="html">more firsts</title><link href="http://localhost:4000/2019/06/18/title.html" rel="alternate" type="text/html" title="more firsts" /><published>2019-06-18T21:01:00+02:00</published><updated>2019-06-18T21:01:00+02:00</updated><id>http://localhost:4000/2019/06/18/title</id><content type="html" xml:base="http://localhost:4000/2019/06/18/title.html">&lt;p&gt;The PhD school at DTU Compute organized a PhD Bazaar. It was a great opportunity to learn about everyone’s work and I feel lucky to be part this amazing group of young researchers. My own pitch was caught on  &lt;a href=&quot;https://video.dtu.dk/media/Pola+Schwöbel/0_1890tjat&quot;&gt;video&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;img&quot;&gt;
	&lt;img class=&quot;col three&quot; src=&quot;/img/pitch.png&quot; /&gt;
&lt;/div&gt;
&lt;div class=&quot;col three caption&quot;&gt;
&lt;/div&gt;</content><author><name></name></author><summary type="html">The PhD school at DTU Compute organized a PhD Bazaar. It was a great opportunity to learn about everyone’s work and I feel lucky to be part this amazing group of young researchers. My own pitch was caught on video.</summary></entry><entry><title type="html">a lot of firsts</title><link href="http://localhost:4000/2019/06/17/title.html" rel="alternate" type="text/html" title="a lot of firsts" /><published>2019-06-17T19:53:00+02:00</published><updated>2019-06-17T19:53:00+02:00</updated><id>http://localhost:4000/2019/06/17/title</id><content type="html" xml:base="http://localhost:4000/2019/06/17/title.html">&lt;p&gt;I finally made my website to collect some notes, thoughts and learnings from my PhD. It seems that it’s become increasingly more important to market yourself as a researcher (or, really, as a human being). While many of us are critical of this trend  - and rightfully so - I do think there is value in learning how to communicate your work.&lt;/p&gt;

&lt;p&gt;So I went ahead and made my own website! I used github.pages and the jekyll templating engine. The upsides: It’s free, and rather easy to do if you have some basic coding skills. It should also make it very easy to publish tutorials and all things code - we’ll see about this later. The downside: While there is templates out there to use, it probably won’t turn out quite as beautiful as with services like squarespace or cargo collective (at least not with my newly acquired and very mediocre web development hacker skills).&lt;/p&gt;</content><author><name></name></author><summary type="html">I finally made my website to collect some notes, thoughts and learnings from my PhD. It seems that it’s become increasingly more important to market yourself as a researcher (or, really, as a human being). While many of us are critical of this trend - and rightfully so - I do think there is value in learning how to communicate your work.</summary></entry></feed>